<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>project icps</title>

  <link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="../css/sub-page.css">

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-QZ8WNGYXYL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-QZ8WNGYXYL');
  </script>

</head>
<body>
  <main>
    <h1>
        A User Interface for Sense-making of the Reasoning Process while Interacting with Robots
    </h1>
    <!-- <iframe width="560" height="315" src="https://www.youtube.com/embed/c7vojX7NJjg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
    <p>
        Previously, graph visualization has been used wildly by developers to make sense of knowledge representations. 
        However, due to lacking the link between abstract knowledge of the real-world environment and the robot's actions, traditional visualization tools are incompatible for expert-user to understand, test, supervise and modify the graph-based reasoning system with the embodiment of the robots. 
        Therefore, we developed an interface which enables robotic experts to send commands to the robot in natural language, then interface visualizes the procedures of the robot mapping the command to the functions for querying in the commonsense knowledge database, links the result to the real world instances in a 3D map and demonstrate the execution of the robot from the first-person perspective of the robot. After 3 weeks of usage of the system by robotic experts in their daily development, some feedback was collected, which provides insight for designing such systems.     
    </p>

    <h2>
      Universial Access
    </h2>
    <img src="../img/subpages/kore/scene.png" class="zoom-pic">
    <p>
        A web-based graphical user interface (GUI) is designed and implemented. Expert users can visit the interface anywhere at Honda Research Institute with any device. The dialogue box of the interface allows the user to type in commands to the robot in natural language and receive the answer from the robot. 
        Users can switch between camera-mode, graph-mode and map-mode.
    </p>
    
    <h2>
      Design of the AR interface
    </h2>
    <img src="../img/subpages/kore/cam_mode.png" class="zoom-pic">

    <h2>
        User Interface: Camera-mode
    </h2>
    <img src="../img/subpages/kore/graph_mode.png" class="zoom-pic">

    <h2>
        System structure
      </h2>
    <img src="../img/subpages/kore/map_mode.png" class="zoom-pic">

    <h2>
      Publications:
    </h2>
    <a href="https://arxiv.org/abs/2210.08246" target="_blank">
        Wang, C., & Deigmoeller, J. (2022). A User Interface for Sense-making of the Reasoning Process while Interacting with Robots. arXiv preprint arXiv:2210.08246.
    </a>

  </main>

</body>
</html>