<!DOCTYPE html>
<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../css/sub-page.css">

  </head>
  <body>
    <main>
      <h1> Human-Vehicle Cooperation on Prediction-Level</h1>
      <iframe width="560" height="315" src="https://www.youtube.com/embed/xelY4qw0CQI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      <!-- <img src="../img/subpages/preAD/title.png">  -->
      <p>
        It seems that autonomous driving systems are substituting human responsibilities in the driving task.
        However, this does not mean that vehicles should not interact with their driver anymore, even in case of full automation. 
        One reason is that the automation is not yet advanced enough to predict other road user's behavior in complex situations, which can lead to sub-optimal action choices, decrease comfort and user experience. 
        In contrast, a human driver may have a more reliable understanding of other road users' intentions which could complement that of the automation. 
        We implement an approach that lets a human driver quickly and intuitively supplement scene predictions to an autonomous driving system by gaze.
      </p>
      <img src="../img/subpages/preAD/before_intervention.jpg">
      <img src="../img/subpages/preAD/after_intervention.jpg"> 

      <figure>
        <img src="../img/subpages/preAD/gaze_cal.png"> 
        <figcaption>The gaze calculation method</figcaption>  
      </figure>

      <img src="../img/subpages/preAD/procedure.png"> 

      <p>
        The interaction flow of the second scenario: 1) A sports car is driving behind a truck. 2) The system does not predict the car to change its lane 3) The driver clicks the button while gazing on the sports car indicating that it may change lane. 4) After the system has received the driver’s input: a sound feedback is provided; the sports car is highlighted in red in the GUI; iTFA changes the ego vehicle's planned trajectory to a lane change. 5) The system maneuvers the vehicle accordingly.
      </p>

      <iframe width="560" height="315" src="https://www.youtube.com/embed/RphavtH7Wpo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      <p>
        Full video description
      </p>

      <h2>
        Publications:
      </h2>
      <a href="https://arxiv.org/abs/2104.03019" target="_blank">
        Wang, C., Weisswange, T. H., Krueger, M., & Wiebel-Herboth, C. B. (2021). Human-Vehicle Cooperation on Prediction-Level: Enhancing Automated Driving with Human Foresight. arXiv preprint arXiv:2104.03019. Accepted by IEEE Intelligent Vehicles Symposium (IV21)
      </a>
      <a href="https://dl.acm.org/doi/abs/10.1145/3409120.3410652" target="_blank">
        Wang, C., Krüger, M., & Wiebel-Herboth, C. B. (2020, September). “Watch out!”: Prediction-Level Intervention for Automated Driving. In 12th International Conference on Automotive User Interfaces and Interactive Vehicular Applications (pp. 169-180).
      </a>
    </main>
  </body>
</html>